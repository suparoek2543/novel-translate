from google import genai
from google.genai import types
import cloudscraper
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random
import json # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
# ==========================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 
NOVEL_MAIN_URL = "https://kakuyomu.jp/works/822139841708705081"

# ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢ (Database)
JSON_DB_FILE = "novels.json"
HISTORY_FILE = "history_novel_2.txt"

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Client
if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
    except Exception as e:
        print(f"‚ùå Error initializing Client: {e}")
        client = None
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY")
    client = None

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON
# ==========================================

def save_to_json(ep_data):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON"""
    data = []
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if os.path.exists(JSON_DB_FILE):
        with open(JSON_DB_FILE, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except:
                data = [] # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
    
    # 2. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏° (Update ‡∏´‡∏£‡∏∑‡∏≠ Append)
    # ‡πÉ‡∏ä‡πâ Link ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÑ‡∏´‡∏°
    existing_idx = next((index for (index, d) in enumerate(data) if d["link"] == ep_data["link"]), None)
    
    if existing_idx is not None:
        data[existing_idx] = ep_data # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°
    else:
        data.append(ep_data) # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≠‡∏ô‡πÉ‡∏´‡∏°‡πà
        
    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    with open(JSON_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á {JSON_DB_FILE} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)
# ==========================================

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def save_to_history(url):
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def get_first_episode_url():
    print(f"üìñ ‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏à‡∏≤‡∏Å: {NOVEL_MAIN_URL}")
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        first_ep_link = soup.select_one('a#readFromFirstEpisode')
        if first_ep_link: return urljoin(NOVEL_MAIN_URL, first_ep_link['href'])
        else:
            target_pattern = re.compile(r'/works/\d+/episodes/\d+')
            links = soup.find_all('a', href=target_pattern)
            if links:
                sorted_links = sorted(links, key=lambda x: int(re.search(r'episodes/(\d+)', x['href']).group(1)))
                return urljoin(NOVEL_MAIN_URL, sorted_links[0]['href'])
        return None
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def find_next_link(soup, current_url):
    next_btn = soup.select_one('a.widget-episode-navigation-next') or \
               soup.select_one('a#contentMain-readNextEpisode') or \
               soup.find('a', string=re.compile('Ê¨°„ÅÆ„Ç®„Éî„ÇΩ„Éº„Éâ'))
    if next_btn: return urljoin(current_url, next_btn['href'])
    return None

def get_content_and_next_link(url, max_retries=3):
    headers = {'Referer': NOVEL_MAIN_URL}
    for attempt in range(max_retries):
        try:
            time.sleep(random.uniform(1, 1.5)) 
            response = scraper.get(url, headers=headers, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.select_one('.widget-episodeTitle').text.strip()
                body = soup.select_one('.widget-episodeBody').get_text(separator="\n", strip=True)
                next_link = find_next_link(soup, url)
                
                # ‡∏î‡∏∂‡∏á ID ‡∏ï‡∏≠‡∏ô‡∏à‡∏≤‡∏Å URL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                ep_id_match = re.search(r'episodes/(\d+)', url)
                ep_id = ep_id_match.group(1) if ep_id_match else "0"

                return {"title": title, "content": body, "next_link": next_link, "ep_id": ep_id}
            time.sleep(2)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")
    return None

def translate_smart(text, retry_count=0):
    if not client: return None, "No Client"
    if not text: return None, "No Content"
    
    if retry_count == 0:
        prompt = f"‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏ô‡∏∏‡∏Å:\n- ‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏≥\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}"
    elif retry_count == 1:
        prompt = f"**‡πÅ‡∏õ‡∏•‡πÇ‡∏î‡∏¢‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡πÄ‡∏û‡∏®/‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á**\n- ‡∏™‡∏£‡∏∏‡∏õ‡∏â‡∏≤‡∏Å‡∏ß‡∏≤‡∏ö‡∏´‡∏ß‡∏¥‡∏ß‡πÅ‡∏ó‡∏ô\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}"
    else:
        prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text[:15000]}"

    try:
        response = client.models.generate_content(
            model='gemini-2.5-pro', contents=prompt,
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        if not response.text or not response.text.strip(): raise ValueError("Empty response")
        return response.text, None 
    except Exception as e:
        error_msg = str(e)
        if ("429" in error_msg or "503" in error_msg):
            time.sleep((retry_count + 1) * 10)
            return translate_smart(text, retry_count) 
        elif retry_count < 2:
            time.sleep(2)
            return translate_smart(text, retry_count + 1)
        else:
            return None, f"‡∏¢‡∏≠‡∏°‡πÅ‡∏û‡πâ ({error_msg})"

# ==========================================
# üöÄ Main Loop
# ==========================================

def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Web Novel Generator...")
    completed_urls = load_history()
    current_url = get_first_episode_url()
    if not current_url: return

    ep_count = 1
    
    while current_url:
        print(f"\n[{ep_count}] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {current_url}")
        
        if current_url in completed_urls:
            print("   ‚è© ‡∏°‡∏µ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°")
            data = get_content_and_next_link(current_url)
            if data and data['next_link']:
                current_url = data['next_link']
                ep_count += 1
                continue
            else:
                print("   üèÅ ‡∏à‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏°)")
                break

        data = get_content_and_next_link(current_url)
        if not data:
            print("   ‚ùå ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ -> ‡∏à‡∏ö")
            break

        title = data['title']
        translated, error_msg = translate_smart(data['content'])
        
        if translated:
            print("   ‚úÖ ‡πÅ‡∏õ‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á JSON")
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
            episode_data = {
                "ep_id": data['ep_id'],
                "title": title,
                "content": translated,
                "link": current_url
            }
            
            # üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
            save_to_json(episode_data)
            save_to_history(current_url)
            completed_urls.add(current_url)
        else:
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {error_msg}")

        if data['next_link']:
            print(f"   ‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏û‡∏±‡∏Å 5 ‡∏ß‡∏¥)")
            current_url = data['next_link']
            ep_count += 1
            time.sleep(5) 
        else:
            print("\nüèÅ ‡∏à‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß")
            current_url = None

    print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")

if __name__ == "__main__":
    main()
