from google import genai
from google.genai import types
import cloudscraper
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random
import json
from urllib.parse import urljoin

# ==========================================
# ‚öôÔ∏è ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
# ==========================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 
NOVEL_MAIN_URL = "https://kakuyomu.jp/works/822139839754922306"

JSON_DB_FILE = "novels.json"
HISTORY_FILE = "history_novel_2.txt"

if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
    except:
        client = None
else:
    client = None

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏õ‡∏• (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)
# ==========================================

def translate_title(text):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á/‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à"""
    if not client or not text: return text # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Key ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
    
    prompt = f"""
    Translate this Japanese novel title to Thai.
    Target audience: Teenagers / Light Novel readers.
    Style: Catchy, Short, Natural.
    
    Strict Rules:
    1. Output ONLY the translated text.
    2. No explanations, no notes, no options.
    
    Original Text:
    {text}
    """
    try:
        # ‡πÉ‡∏ä‡πâ Flash Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß
        response = client.models.generate_content(
            model='gemini-2.5-pro', 
            contents=prompt,
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        if response.text:
            return response.text.strip().replace("‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:", "").replace("‡πÅ‡∏õ‡∏•:", "").strip()
        return text
    except:
        return text # ‡∏ñ‡πâ‡∏≤ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON
# ==========================================

def get_novel_title():
    """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢ + ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢"""
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å: {NOVEL_MAIN_URL}")
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title_elem = soup.select_one('#workTitle') or soup.select_one('h1')
        raw_title = title_elem.text.strip() if title_elem else "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"
        
        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        thai_title = translate_title(raw_title)
        
        print(f"‚úÖ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢: {thai_title} (Original: {raw_title})")
        return thai_title
    except Exception as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        return "‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠"

def save_to_json(novel_title, ep_data):
    data = {}
    if os.path.exists(JSON_DB_FILE):
        with open(JSON_DB_FILE, "r", encoding="utf-8") as f:
            try:
                content = f.read()
                if content: data = json.loads(content)
                if isinstance(data, list): data = {}
            except: data = {}

    novel_id = NOVEL_MAIN_URL
    
    if novel_id not in data:
        data[novel_id] = { "title": novel_title, "chapters": [] }
    
    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•)
    data[novel_id]["title"] = novel_title
    
    chapters = data[novel_id]["chapters"]
    existing_idx = next((index for (index, d) in enumerate(chapters) if d["link"] == ep_data["link"]), None)
    
    if existing_idx is not None:
        chapters[existing_idx] = ep_data
    else:
        chapters.append(ep_data)
        
    data[novel_id]["chapters"] = chapters

    with open(JSON_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {ep_data['ep_id']} ‡∏•‡∏á JSON ‡πÅ‡∏•‡πâ‡∏ß")

# ==========================================
# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Crawler (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
# ==========================================

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f: return set(l.strip() for l in f)

def save_to_history(url):
    with open(HISTORY_FILE, "a", encoding="utf-8") as f: f.write(url + "\n")

def get_first_episode_url():
    try:
        response = scraper.get(NOVEL_MAIN_URL)
        soup = BeautifulSoup(response.text, 'html.parser')
        l = soup.select_one('a#readFromFirstEpisode')
        if l: return urljoin(NOVEL_MAIN_URL, l['href'])
        ts = re.compile(r'/works/\d+/episodes/\d+')
        ls = soup.find_all('a', href=ts)
        if ls: 
            sl = sorted(ls, key=lambda x: int(re.search(r'episodes/(\d+)', x['href']).group(1)))
            return urljoin(NOVEL_MAIN_URL, sl[0]['href'])
    except: pass
    return None

def find_next_link(soup, url):
    n = soup.select_one('a.widget-episode-navigation-next') or soup.select_one('a#contentMain-readNextEpisode') or soup.find('a', string=re.compile('Ê¨°„ÅÆ„Ç®„Éî„ÇΩ„Éº„Éâ'))
    return urljoin(url, n['href']) if n else None

def get_content_and_next_link(url, max=3):
    h={'Referer': NOVEL_MAIN_URL}
    for _ in range(max):
        try:
            time.sleep(1)
            r = scraper.get(url, headers=h, timeout=15)
            if r.status_code==200:
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.select_one('.widget-episodeTitle').text.strip()
                b = s.select_one('.widget-episodeBody').get_text(separator="\n", strip=True)
                eid = re.search(r'episodes/(\d+)', url).group(1)
                return {"title":t, "content":b, "next_link":find_next_link(s, url), "ep_id":eid}
        except: time.sleep(2)
    return None

def translate_smart(text, retry_count=0):
    if not client or not text: return None, "Error"
    
    # -----------------------------------------------------------
    # üõ°Ô∏è ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏• 3 ‡∏£‡∏∞‡∏î‡∏±‡∏ö (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ)
    # -----------------------------------------------------------
    if retry_count == 0:
        # ‡∏£‡∏∞‡∏î‡∏±‡∏ö 1: ‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô)
        prompt = f"""
        ‡πÅ‡∏õ‡∏•‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô:
        - ‡πÅ‡∏õ‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£
        - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏° ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏ó‡∏ô
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:
        {text[:15000]}
        """
    elif retry_count == 1:
        # ‡∏£‡∏∞‡∏î‡∏±‡∏ö 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡∏£‡∏¢)
        print("   ‚ö†Ô∏è ‡∏ï‡∏¥‡∏î Safety... ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î: Soften (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏≥)")
        prompt = f"""
        **‡πÅ‡∏õ‡∏•‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡∏•‡∏á (Soft Version)**
        - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏•‡∏≤‡∏°‡∏Å‡∏≠‡∏ô‡∏≤‡∏à‡∏≤‡∏£‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
        - ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏â‡∏≤‡∏Å‡∏£‡πà‡∏ß‡∏°‡πÄ‡∏û‡∏® ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡∏£‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥
        - ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:
        {text[:15000]}
        """
    else:
        # ‡∏£‡∏∞‡∏î‡∏±‡∏ö 3: ‡πÑ‡∏°‡πâ‡∏ï‡∏≤‡∏¢ (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô PG-13)
        print("   ‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô... ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î: Clean Version (‡∏ï‡∏±‡∏î‡∏â‡∏≤‡∏Å‡πÄ‡∏£‡∏ó)")
        prompt = f"""
        **‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (All Ages Version)**
        - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        - **‡∏ï‡∏±‡∏î‡∏â‡∏≤‡∏Å 18+ ‡∏ó‡∏¥‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏î** ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
        - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏≤‡∏á‡πÄ‡∏û‡∏® ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ß‡πà‡∏≤ "‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡πà‡∏≥‡∏Ñ‡∏∑‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô" ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏â‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
        - ‡∏Ç‡∏≠‡πÅ‡∏Ñ‡πà‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πá‡∏û‡∏≠
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:
        {text[:15000]}
        """

    try:
        # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• 1.5-flash (‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏ñ‡∏∂‡∏Å‡∏Å‡∏ß‡πà‡∏≤ pro ‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡πÄ‡∏Ñ‡∏™)
        response = client.models.generate_content(
            model='gemini-2.5-pro', 
            contents=prompt,
            config=types.GenerateContentConfig(safety_settings=[
                types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_NONE'),
                types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_NONE'), # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Block None
                types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_NONE')
            ])
        )
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÅ‡∏õ‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
        if not response.text or not response.text.strip():
            raise ValueError("Empty response (Safety Blocked)")
            
        return response.text, None 

    except Exception as e:
        error_msg = str(e)
        
        # ‡∏ñ‡πâ‡∏≤ Server ‡πÄ‡∏ï‡πá‡∏° ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        if "429" in error_msg:
             print(f"   ‚ö†Ô∏è Server Busy. ‡∏£‡∏≠ 10 ‡∏ß‡∏¥...")
             time.sleep(10)
             return translate_smart(text, retry_count)
        
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö 3 ‡∏£‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        if retry_count < 2: 
            time.sleep(2)
            return translate_smart(text, retry_count + 1)
            
        # üö® ‡∏ñ‡πâ‡∏≤‡∏•‡∏≠‡∏á 3 ‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ (Google ‡∏ö‡∏•‡πá‡∏≠‡∏Ñ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ö‡∏±‡∏ç‡∏ä‡∏µ)
        # ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ Error (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÑ‡∏î‡πâ)
        print("   ‚ùå ‡∏¢‡∏≠‡∏°‡πÅ‡∏û‡πâ: ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏¢‡∏µ‡∏¢‡∏ß‡∏¢‡∏≤")
        fallback_text = (
            "‚ö†Ô∏è **[‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏ö]** ‚ö†Ô∏è\n\n"
            "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà AI ‡∏à‡∏∞‡∏¢‡∏≠‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• "
            "‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ\n"
            "(‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏î‡∏•‡∏¥‡∏á‡∏Å‡πå Original ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)"
        )
        return fallback_text, None
# ==========================================
# üöÄ Main Loop
# ==========================================

def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Web Novel (‡πÅ‡∏õ‡∏•‡πÑ‡∏ó‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)...")
    
    # 1. ‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
    novel_title = get_novel_title()
    
    completed_urls = load_history()
    current_url = get_first_episode_url()
    
    if not current_url: 
        print("‚ùå ‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠"); return

    ep_count = 1
    
    while current_url:
        print(f"\n[{ep_count}] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {current_url}")
        
        if current_url in completed_urls:
            print("   ‚è© ‡∏°‡∏µ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß -> ‡∏Ç‡πâ‡∏≤‡∏°")
            data = get_content_and_next_link(current_url) 
            if data and data['next_link']:
                current_url = data['next_link']
                ep_count += 1
                continue
            else:
                break

        data = get_content_and_next_link(current_url)
        if not data: break

        # üü¢ ‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        print(f"   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ô: {data['title']}")
        thai_chapter_title = translate_title(data['title'])
        
        print("   ‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...")
        translated_content, err = translate_smart(data['content'])
        
        if translated_content:
            print(f"   ‚úÖ ‡πÅ‡∏õ‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            
            ep_data = {
                "ep_id": data['ep_id'],
                "title": thai_chapter_title, # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß
                "content": translated_content,
                "link": current_url
            }
            
            save_to_json(novel_title, ep_data)
            save_to_history(current_url)
            completed_urls.add(current_url)
        else:
            print(f"   ‚ùå ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {err}")

        if data['next_link']:
            print("   ‚û°Ô∏è ‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
            current_url = data['next_link']
            ep_count += 1
            time.sleep(5)
        else:
            print("üèÅ ‡∏à‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
            break

if __name__ == "__main__":
    main()
